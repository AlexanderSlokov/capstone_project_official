import csv
import os
import socket
import sys
import threading

import streamlit as st
import torch
import unicodedata
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma

from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from localGPT_app.run_localGPT import load_model
from scripts.env_checking import system_check

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """B·∫°n l√† tr·ª£ l√Ω c√≥ quy·ªÅn truy c·∫≠p v√†o c√°c t√†i li·ªáu ng·ªØ c·∫£nh c·ª• th·ªÉ. Ch·ªâ tr·∫£ l·ªùi c√°c
c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát, d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. N·∫øu b·∫°n kh√¥ng th·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh,
h√£y th√¥ng b√°o cho ng∆∞·ªùi d√πng m·ªôt c√°ch l·ªãch s·ª±. Kh√¥ng s·ª≠ d·ª•ng b·∫•t k·ª≥ th√¥ng tin b√™n ngo√†i n√†o."""


# ===========================================
# 1. Ki·ªÉm tra quy·ªÅn truy c·∫≠p (M·∫≠t kh·∫©u v√† IP)
# ===========================================


# Tr·∫°ng th√°i nh·∫≠p m·∫≠t kh·∫©u ƒë√∫ng ho·∫∑c sai
def check_password():
    """H√†m ki·ªÉm tra m·∫≠t kh·∫©u v·ªõi logic c·∫£i ti·∫øn."""
    if "password_attempted" not in st.session_state:
        st.session_state["password_attempted"] = False  # Tr·∫°ng th√°i l·∫ßn th·ª≠ m·∫≠t kh·∫©u ƒë·∫ßu ti√™n

    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False

    if not st.session_state["password_correct"]:
        # Hi·ªÉn th·ªã √¥ nh·∫≠p m·∫≠t kh·∫©u
        password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u:", type="password")

        # Ch·ªâ hi·ªÉn th·ªã th√¥ng b√°o sai m·∫≠t kh·∫©u n·∫øu ƒë√£ th·ª≠ √≠t nh·∫•t 1 l·∫ßn
        if password:
            st.session_state["password_attempted"] = True
            if password == "r":  # Thay b·∫±ng m·∫≠t kh·∫©u th·ª±c t·∫ø
                st.session_state["password_correct"] = True

        if st.session_state["password_attempted"] and not st.session_state["password_correct"]:
            st.error("M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i.")

        return False  # Ch∆∞a x√°c th·ª±c

    return True  # ƒê√£ x√°c th·ª±c


def get_client_ip():
    """L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa host."""
    try:
        # Tr·∫£ v·ªÅ ƒë·ªãa ch·ªâ IP c·ªßa host trong m·∫°ng LAN
        hostname = socket.gethostname()
        client_ip = socket.gethostbyname(hostname)
        return client_ip
    except Exception as get_client_ip_exeption:
        return f"Unknown IP ({get_client_ip_exeption})"


def check_ip_whitelist():
    """H√†m ki·ªÉm tra IP ng∆∞·ªùi d√πng v·ªõi logic c·∫£i ti·∫øn."""
    allowed_ips = ["172.25.224.1", "127.0.0.1"]  # Th√™m IP cho ph√©p t·∫°i ƒë√¢y
    client_ip = get_client_ip()

    if client_ip not in allowed_ips:
        st.error(f"Truy c·∫≠p b·ªã t·ª´ ch·ªëi: IP c·ªßa b·∫°n kh√¥ng ƒë∆∞·ª£c ph√©p truy c·∫≠p.")
        st.stop()


# X√°c th·ª±c m·∫≠t kh·∫©u tr∆∞·ªõc
if not check_password():
    st.stop()

# Ki·ªÉm tra quy·ªÅn truy c·∫≠p theo IP sau khi nh·∫≠p ƒë√∫ng m·∫≠t kh·∫©u
check_ip_whitelist()


# ========================================
# 2. T·∫°o PromptTemplate cho c√°c lo·∫°i model
# ========================================

def create_prompt_template(system_prompt_setup=system_prompt, prompt_template_type=None, history=False):
    if prompt_template_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif prompt_template_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        if history:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif prompt_template_type == "qwen":
        # C·∫•u tr√∫c prompt cho Qwen
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    else:
        # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
        if history:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return (
        prompt,
        memory,
    )


# ======================================
# 3. C√°c h√†m ti·ªán √≠ch chung
# ======================================


# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]


# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


def clean_response(response_text):
    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng mong mu·ªën
    unwanted_tags = ["[OUT]", "[INVISIBLE TEXT]", "<<BEGIN>>", "<<END>>"]
    for tag in unwanted_tags:
        response_text = response_text.replace(tag, "")
    # X√≥a c√°c kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    response_text = response_text.strip()
    return response_text


# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"

# =======================================
# H√†m cache ƒë·ªÉ t·∫£i m·ªôt l·∫ßn duy nh·∫•t
# =======================================
model_lock = threading.Lock()


@st.cache_resource
def load_cached_model():
    llm_model = load_model(  # ƒê·ªïi t√™n `llm` th√†nh `llm_model` ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n v·ªõi bi·∫øn to√†n c·ª•c
        device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME
    )
    return llm_model


@st.cache_resource
def initialize_embeddings_and_db():
    # S·ª≠ d·ª•ng t√™n r√µ r√†ng h∆°n cho c√°c bi·∫øn
    hf_embeddings = HuggingFaceInstructEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": DEVICE_TYPE},
        embed_instruction="Represent the document content for retrieval in Vietnamese:",
        query_instruction="Represent the query content for retrieval in Vietnamese:",
    )

    chroma_db = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=hf_embeddings,
        client_settings=CHROMA_SETTINGS,
    )
    return hf_embeddings, chroma_db


@st.cache_resource
def create_cached_retriever(_hf_embeddings, _chroma_db, _method_type, _top_k, _fetch_k, _score_threshold=None,
                            _mmr_lambda=None):
    # ƒê·ªïi t√™n `retriever` th√†nh `cached_retriever` ƒë·ªÉ kh√¥ng shadow bi·∫øn ngo√†i h√†m
    cached_retriever = None  # ƒê·∫£m b·∫£o kh·ªüi t·∫°o bi·∫øn v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
    if _method_type == "similarity":
        cached_retriever = _chroma_db.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": _top_k,
                "fetch_k": _fetch_k,
                "score_threshold": _score_threshold,
            },
        )
    elif _method_type == "mmr":
        cached_retriever = _chroma_db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": _top_k,
                "fetch_k": _fetch_k,
                "lambda": _mmr_lambda,
            },
        )
    return cached_retriever


@st.cache_data
@st.cache_data
def perform_environment_check():
    """Th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng th√¥ng qua system_check v√† l∆∞u k·∫øt qu·∫£."""
    # G·ªçi ƒë·∫øn system_check() ƒë·ªÉ th·ª±c hi·ªán t·∫•t c·∫£ c√°c ki·ªÉm tra
    results = system_check()
    return results


# ============================================
# 4. Ph·∫ßn ch√≠nh c·ªßa giao di·ªán ·ª©ng d·ª•ng Streamlit
# ============================================


# Sidebar b√™n c·∫°nh tr√°i.
# Sidebar for system environment checks
with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n.")
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra h·ªá th·ªëng n·∫øu ch∆∞a c√≥ k·∫øt qu·∫£ trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()

    env_results = st.session_state["env_results"]  # L·∫•y k·∫øt qu·∫£ ƒë√£ l∆∞u

    # Hi·ªÉn th·ªã th√¥ng tin m√¥i tr∆∞·ªùng
    if env_results:
        st.write(f"**CUDA kh·∫£ d·ª•ng:** {'C√≥' if env_results.get('CUDA Available') else 'Kh√¥ng'}")
        st.write(f"**Phi√™n b·∫£n CUDA:** {env_results.get('CUDA Version', 'Kh√¥ng x√°c ƒë·ªãnh')}")

        st.write(f"**VRAM:** {env_results['VRAM']:.2f} GB" if env_results.get("VRAM") else "Kh√¥ng x√°c ƒë·ªãnh")
        st.write(f"**RAM:** {env_results['RAM']:.2f} GB" if env_results.get('RAM') else "Kh√¥ng x√°c ƒë·ªãnh")

        st.write(f"**CPU:** {env_results['CPU'].get('CPU', 'Kh√¥ng x√°c ƒë·ªãnh')}")
        st.write(
            f"**Intel Hyper-Threading:** {'C√≥' if env_results['CPU'].get('Intel Hyper-Threading') else 'Kh√¥ng'}"
        )
        st.write(f"**CUDA Compute Capability:** {env_results.get('CUDA Capability', 'Kh√¥ng x√°c ƒë·ªãnh')}")

        st.write(f"**ƒê∆∞·ªùng d·∫´n Conda CUDA:** {env_results.get('Conda Path', 'Kh√¥ng t√¨m th·∫•y')}")
        st.write(f"**Phi√™n b·∫£n Conda CUDA:** {env_results.get('Conda Version', 'Kh√¥ng t√¨m th·∫•y')}")
        st.write(f"**NVCC Version:** {env_results.get('NVCC Version', 'Kh√¥ng t√¨m th·∫•y')}")

        # Hi·ªÉn th·ªã y√™u c·∫ßu m√¥ h√¨nh
        model_req = env_results.get("Model Requirements", {})
        st.subheader("üß† Y√™u c·∫ßu m√¥ h√¨nh:")
        if model_req:
            st.write(f"**VRAM c·∫ßn thi·∫øt:** {model_req.get('VRAM Required', 'Kh√¥ng x√°c ƒë·ªãnh')} GB")
            st.write(f"**RAM c·∫ßn thi·∫øt:** {model_req.get('RAM Required', 'Kh√¥ng x√°c ƒë·ªãnh')} GB")
            st.write(f"**VRAM ƒë·ªß:** {'C√≥' if model_req.get('Sufficient VRAM') else 'Kh√¥ng'}")
            st.write(f"**RAM ƒë·ªß:** {'C√≥' if model_req.get('Sufficient RAM') else 'Kh√¥ng'}")
            st.write(f"**L·ªõp GPU ƒë·ªÅ xu·∫•t:** {model_req.get('Suggested GPU Layers', 'Kh√¥ng x√°c ƒë·ªãnh')}")
            st.write(f"**K√≠ch th∆∞·ªõc batch ƒë·ªÅ xu·∫•t:** {model_req.get('Suggested Batch Size', 'Kh√¥ng x√°c ƒë·ªãnh')}")
        else:
            st.write("Kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ ki·ªÉm tra y√™u c·∫ßu m√¥ h√¨nh.")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng!")

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát Ô∏èb·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    # st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    # st.markdown("- [Streamlit](https://streamlit.io/) - Framework x√¢y d·ª±ng ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    # st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng h·ªá th·ªëng LLM hi·ªáu qu·∫£.")
    # st.markdown("- [HuggingFace](https://huggingface.co/) - C·ªông ƒë·ªìng ph√°t tri·ªÉn m√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    # st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    # st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    # add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# ============================================
# 4. T·∫£i m√¥ h√¨nh v√† c√°c t√†i nguy√™n
# ============================================
try:
    # Load model and embeddings/database
    st.session_state["llm"] = load_cached_model()
    st.success("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i v√† s·∫µn s√†ng s·ª≠ d·ª•ng!")

    # Initialize embeddings and database
    if "embeddings" not in st.session_state or "db" not in st.session_state:
        embeddings, db = initialize_embeddings_and_db()
        st.session_state["embeddings"] = embeddings
        st.session_state["db"] = db

    # Giao di·ªán ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n
    retrieval_method = st.radio(
        "Ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n:",
        ["similarity - t√¨m th√¥ng tin t∆∞∆°ng t·ª±", "mmr - t√¨m th√¥ng tin li√™n quan"],
    )
    method_type = retrieval_method.split(" - ")[0]

    top_k = st.slider(
        "S·ªë l∆∞·ª£ng t√†i li·ªáu t∆∞∆°ng t·ª± ƒë∆∞·ª£c tr·∫£ v·ªÅ (top_k):",
        min_value=1,
        max_value=50,
        value=20,
        step=1,
    )

    fetch_k = st.slider(
        "Ph·∫°m vi t√¨m ki·∫øm (fetch_k):",
        min_value=10,
        max_value=100,
        value=50,
        step=10,
    )

    retriever = None
    if method_type == "similarity":
        input_score_threshold = st.slider(
            "Ng∆∞·ª°ng ƒëi·ªÉm t∆∞∆°ng t·ª± (score_threshold):",
            min_value=0.0,
            max_value=1.0,
            value=0.75,
            step=0.05,
        )
        retriever = create_cached_retriever(
            _hf_embeddings=st.session_state["embeddings"],  # ƒê·ªïi t√™n tham s·ªë
            _chroma_db=st.session_state["db"],
            _method_type=method_type,
            _top_k=top_k,
            _fetch_k=fetch_k,
            _score_threshold=input_score_threshold,
        )
    elif method_type == "mmr":
        mmr_lambda = st.slider(
            "Tr·ªçng s·ªë lambda (MMR):",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1,
        )
        retriever = create_cached_retriever(
            _hf_embeddings=st.session_state["embeddings"],  # ƒê·ªïi t√™n tham s·ªë
            _chroma_db=st.session_state["db"],
            _method_type=method_type,
            _top_k=top_k,
            _fetch_k=fetch_k,
            _mmr_lambda=mmr_lambda,
        )

    # Kh·ªüi t·∫°o QA Chain n·∫øu ch∆∞a c√≥
    if "QA" not in st.session_state or st.session_state["retriever"] != retriever:
        prompt, memory = create_prompt_template(
            system_prompt_setup=system_prompt, prompt_template_type="qwen", history=False
        )
        st.session_state["QA"] = RetrievalQA.from_chain_type(
            llm=st.session_state["llm"],
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
        st.session_state["retriever"] = retriever
        st.success("QA Chain ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!")
except Exception as e:
    st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh ho·∫∑c kh·ªüi t·∫°o QA Chain: {str(e)}")
    st.stop()

# ==========================================
# 5. X·ª≠ l√Ω ƒë·∫ßu v√†o v√† hi·ªÉn th·ªã k·∫øt qu·∫£
# ==========================================
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) ƒë·ªÉ h·ªá th·ªëng truy v·∫•n c√≥ th√™m d·ªØ ki·ªán v√† t√¨m ki·∫øm ch√≠nh x√°c h∆°n (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, "
    "tu·ª≥ ch·ªçn th√™m.)",
    key="additional_keywords",
)

if st.button("G·ª≠i c√¢u h·ªèi"):
    if user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p
            with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n..."):
                response = st.session_state["QA"](enhanced_query)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            answer, docs = response["result"], response["source_documents"]
            st.write("### C√¢u tr·∫£ l·ªùi:")
            st.write(answer)

            # Hi·ªÉn th·ªã t√†i li·ªáu tham kh·∫£o
            with st.expander("T√†i li·ªáu tham kh·∫£o:"):
                for i, doc in enumerate(docs):
                    st.write(f"**T√†i li·ªáu #{i + 1}:**")
                    st.write(doc.page_content)

            # L∆∞u l·ªãch s·ª≠ tr√≤ chuy·ªán
            if "chat_history" not in st.session_state:
                st.session_state["chat_history"] = []
            st.session_state["chat_history"].append({
                "question": user_query,
                "answer": answer,
                "documents": docs
            })

        except Exception as e:
            st.error(f"C√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i!")


# =========================================
# 7. Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh CSV
# =========================================
def clean_text(text):
    """
    Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá v√† thay th·∫ø kho·∫£ng tr·∫Øng kh√¥ng chu·∫©n b·∫±ng kho·∫£ng tr·∫Øng th√¥ng th∆∞·ªùng.
    """
    cleaned_text = []
    for idx, char in enumerate(text):
        # Ki·ªÉm tra n·∫øu k√Ω t·ª± thu·ªôc d·∫°ng kho·∫£ng tr·∫Øng kh√¥ng chu·∫©n
        if char.isspace() or unicodedata.category(char) == 'Zs':
            cleaned_text.append(' ')  # Thay b·∫±ng kho·∫£ng tr·∫Øng th√¥ng th∆∞·ªùng
        elif char.isprintable() and unicodedata.category(char)[0] not in ['C']:
            cleaned_text.append(char)  # Gi·ªØ l·∫°i k√Ω t·ª± h·ª£p l·ªá
        else:
            print(f"K√Ω t·ª± kh√¥ng h·ª£p l·ªá t·∫°i v·ªã tr√≠ {idx}: {repr(char)}")
            cleaned_text.append(' ')  # Thay th·∫ø k√Ω t·ª± l·ªói b·∫±ng kho·∫£ng tr·∫Øng
    return ''.join(cleaned_text)


if st.button("Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh CSV"):
    if "chat_history" in st.session_state:
        try:
            # Chu·∫©n h√≥a d·ªØ li·ªáu
            raw_data = st.session_state["chat_history"]
            formatted_chat_history = [
                (clean_text(item["question"]), clean_text(item["answer"]))
                for item in raw_data
                if "question" in item and "answer" in item
            ]

            # Ki·ªÉm tra n·∫øu danh s√°ch tr·ªëng
            if not formatted_chat_history:
                raise ValueError("D·ªØ li·ªáu kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng. Kh√¥ng th·ªÉ xu·∫•t CSV.")

            # Ghi v√†o CSV
            log_dir, log_file = "local_chat_history", "qa_log.csv"
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            csv_path = os.path.join(log_dir, log_file)

            with open(csv_path, mode="w", newline="", encoding="utf-8") as file:
                writer = csv.writer(file)
                writer.writerow(["C√¢u h·ªèi", "C√¢u tr·∫£ l·ªùi"])  # Ti√™u ƒë·ªÅ
                writer.writerows(formatted_chat_history)  # D·ªØ li·ªáu

            # Hi·ªÉn th·ªã th√¥ng b√°o v√† n√∫t t·∫£i xu·ªëng
            st.success(f"L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ ƒë∆∞·ª£c xu·∫•t ra CSV: {csv_path}")
            with open(csv_path, "rb") as file:
                st.download_button(
                    label="T·∫£i xu·ªëng CSV",
                    data=file,
                    file_name="chat_history.csv",
                    mime="text/csv"
                )
        except Exception as e:
            st.error(f"L·ªói khi xu·∫•t CSV: {str(e)}")
    else:
        st.error("Kh√¥ng c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ xu·∫•t.")
