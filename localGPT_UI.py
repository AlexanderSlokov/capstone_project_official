import sys
import os

import torch
import streamlit as st
from localGPT_app.run_localGPT import load_model
from langchain.vectorstores import Chroma
from scripts.env_checking import system_check
from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """You are a knowledgeable assistant with access to specific context documents. You must answer the
questions only in Vietnamese language. You must answer questions based on the provided context only.
If you cannot answer based on the context, inform the user politely. Do not use any external information."""

def model_memory(system_prompt_setup=system_prompt, promptTemplate_type=None, history=False):
    if promptTemplate_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        if history:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "qwen":
        # C·∫•u tr√∫c prompt cho Qwen
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    else:
        # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
        if history:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return (
        prompt,
        memory,
    )



# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]

# S·ª≠ d·ª•ng pipeline n√¢ng cao v·ªõi t·ª´ kh√≥a v√† c·∫•u h√¨nh top_k
def initialize_pipeline_with_keywords():
    # T·∫°o embeddings
    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": DEVICE_TYPE})

    # T·∫°o database vector
    db = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings,
        client_settings=CHROMA_SETTINGS,
    )

    # S·ª≠ d·ª•ng retriever v·ªõi top_k v√† t√¨m ki·∫øm similarity
    retriever = db.as_retriever(
        search_kwargs={"k": 10},  # L·∫•y 10 t√†i li·ªáu li√™n quan nh·∫•t
        search_type="similarity",  # S·ª≠ d·ª•ng t√¨m ki·∫øm similarity
    )

    # T·∫°o prompt v√† memory
    prompt, memory = model_memory(promptTemplate_type="qwen", history=False)

    # T·∫£i m√¥ h√¨nh LLM t·ª´ CLI
    llm = load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME)

    # T·∫°o pipeline QA n√¢ng cao
    qa_pipeline = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # C√≥ th·ªÉ ƒë·ªïi th√†nh "map_reduce" ho·∫∑c "refine" n·∫øu c·∫ßn
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt, "memory": memory},
    )

    return qa_pipeline

# ƒê·∫∑t sau c√°c h√†m nh∆∞ initialize_component
def load_model(device_type, model_id, model_basename=None):
    model, tokenizer = load_full_model(model_id, model_basename, device_type, logging)

    # T·∫°o pipeline v·ªõi th√¥ng s·ªë CLI
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=MAX_NEW_TOKENS,
        do_sample=True,
        temperature=0.7,  # Thay ƒë·ªïi th√¥ng s·ªë sinh vƒÉn b·∫£n
        top_p=0.9,
        top_k=40,
        repetition_penalty=1.1,
    )

    local_llm = HuggingFacePipeline(pipeline=pipe)
    return local_llm

# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n. ")
    st.title("B·∫£o m·∫≠t v√† ri√™ng t∆∞, ho√†n to√†n n·ªôi b·ªô.")
    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng c·ªßa:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    # Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
    load_model_flag = st.checkbox("N·∫°p m√¥ h√¨nh AI (Vui l√≤ng b·∫•m ch·ªçn ƒë·ªÉ tri·ªÉn khai m√¥ h√¨nh AI.)", value=False)

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA kh·∫£ d·ª•ng: {'C√≥' if cuda_available else 'Kh√¥ng'}")
        st.write(f"T·ªïng dung l∆∞·ª£ng VRAM: {total_vram:.2f} GB" if total_vram else "Kh√¥ng th·ªÉ l·∫•y th√¥ng tin VRAM")
        st.write(f"Phi√™n b·∫£n CUDA: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát Ô∏èb·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    st.markdown("- [Streamlit](https://streamlit.io/) - Framework x√¢y d·ª±ng ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng h·ªá th·ªëng LLM hi·ªáu qu·∫£.")
    st.markdown("- [HuggingFace](https://huggingface.co/) - C·ªông ƒë·ªìng ph√°t tri·ªÉn m√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")

# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"
# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Initialize embeddings
    EMBEDDINGS = initialize_component(
        "EMBEDDINGS",
        lambda: HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": DEVICE_TYPE})
    )

    # Initialize database
    DB = initialize_component(
        "DB",
        lambda: Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=EMBEDDINGS,
            client_settings=CHROMA_SETTINGS,
        )
    )

    # Initialize retriever
    RETRIEVER = initialize_component("RETRIEVER", lambda: DB.as_retriever())

    # S·ª≠ d·ª•ng h√†m load_model m·ªõi
    LLM = initialize_component("LLM", lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID,
                                                         model_basename=MODEL_BASENAME))

    # S·ª≠ d·ª•ng prompt cho Qwen v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
    prompt, memory = model_memory(promptTemplate_type="qwen", history=False)
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )

    # Initialize QA pipeline
    prompt, memory = model_memory()
    QA = initialize_pipeline_with_keywords()
else:
    st.warning("Qu√° tr√¨nh kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ ƒëang ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y ·ª©ng d·ª•ng. Vui l√≤ng kh·ªüi ƒë·ªông quy tr√¨nh v·ªõi n√∫t *N·∫°p M√¥ H√¨nh AI* ·ªü b·∫£ng tr∆∞·ª£t ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Sidebar t√πy ch·ªânh th√¥ng s·ªë
with st.sidebar:
    top_k = st.number_input("S·ªë t√†i li·ªáu c·∫ßn l·∫•y (Top-k):", min_value=1, max_value=20, value=10)
    additional_keywords = st.text_input("Th√™m t·ª´ kh√≥a (tu·ª≥ ch·ªçn):", value="")

# √Åp d·ª•ng `top_k` v√†o pipeline
retriever = DB.as_retriever(
    search_kwargs={"k": top_k},  # L·∫•y s·ªë t√†i li·ªáu do ng∆∞·ªùi d√πng ch·ªçn
    search_type="similarity",
)

# Th√™m t·ª´ kh√≥a v√†o c√¢u truy v·∫•n
enhanced_query = user_query + " " + additional_keywords if additional_keywords else user_query

# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")

# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if QA is None:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif user_query.strip():
        try:
            # G·ªçi QA v·ªõi truy v·∫•n n√¢ng c·∫•p
            response = QA(enhanced_query)
            answer, docs = response["result"], response["source_documents"]

            # Display the answer
            st.write("### Answer:")
            st.write(answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")
        except Exception as e:
            st.error(f"An error occurred while processing the query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")

