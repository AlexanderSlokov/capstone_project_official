import sys
import os
import socket
import torch
from flask import request
from streamlit.web.server.server import Server
import streamlit as st
from localGPT_app.run_localGPT import load_model
from langchain.vectorstores import Chroma
from scripts.env_checking import system_check
from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªõi quy·ªÅn truy c·∫≠p v√†o c√°c t√†i li·ªáu ng·ªØ c·∫£nh. B·∫°n ph·∫£i tr·∫£ l·ªùi
c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng s·ª≠ d·ª•ng th√¥ng tin b√™n ngo√†i.
"""

QA = None

# ===========================================
# 1. Ki·ªÉm tra quy·ªÅn truy c·∫≠p (M·∫≠t kh·∫©u v√† IP)
# ===========================================


# Tr·∫°ng th√°i nh·∫≠p m·∫≠t kh·∫©u ƒë√∫ng ho·∫∑c sai
def check_password():
    """H√†m ki·ªÉm tra m·∫≠t kh·∫©u v·ªõi logic c·∫£i ti·∫øn."""
    if "password_attempted" not in st.session_state:
        st.session_state["password_attempted"] = False  # Tr·∫°ng th√°i l·∫ßn th·ª≠ m·∫≠t kh·∫©u ƒë·∫ßu ti√™n

    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False

    if not st.session_state["password_correct"]:
        # Hi·ªÉn th·ªã √¥ nh·∫≠p m·∫≠t kh·∫©u
        password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u:", type="password")

        # Ch·ªâ hi·ªÉn th·ªã th√¥ng b√°o sai m·∫≠t kh·∫©u n·∫øu ƒë√£ th·ª≠ √≠t nh·∫•t 1 l·∫ßn
        if password:
            st.session_state["password_attempted"] = True
            if password == "secure_password":  # Thay b·∫±ng m·∫≠t kh·∫©u th·ª±c t·∫ø
                st.session_state["password_correct"] = True

        if st.session_state["password_attempted"] and not st.session_state["password_correct"]:
            st.error("M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i.")

        return False  # Ch∆∞a x√°c th·ª±c

    return True  # ƒê√£ x√°c th·ª±c


def get_client_ip():
    """L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa host."""
    try:
        # Tr·∫£ v·ªÅ ƒë·ªãa ch·ªâ IP c·ªßa host trong m·∫°ng LAN
        hostname = socket.gethostname()
        client_ip = socket.gethostbyname(hostname)
        return client_ip
    except Exception as get_client_ip_exeption:
        return f"Unknown IP ({get_client_ip_exeption})"


def check_ip_whitelist():
    """H√†m ki·ªÉm tra IP ng∆∞·ªùi d√πng v·ªõi logic c·∫£i ti·∫øn."""
    allowed_ips = ["172.25.224.1", "127.0.0.1"]  # Th√™m IP cho ph√©p t·∫°i ƒë√¢y
    client_ip = get_client_ip()

    if client_ip not in allowed_ips:
        st.error(f"Truy c·∫≠p b·ªã t·ª´ ch·ªëi: IP c·ªßa b·∫°n kh√¥ng ƒë∆∞·ª£c ph√©p truy c·∫≠p.")
        st.stop()


# X√°c th·ª±c m·∫≠t kh·∫©u tr∆∞·ªõc
if not check_password():
    st.stop()

# Ki·ªÉm tra quy·ªÅn truy c·∫≠p theo IP sau khi nh·∫≠p ƒë√∫ng m·∫≠t kh·∫©u
check_ip_whitelist()


# ========================================
# 2. T·∫°o PromptTemplate cho c√°c lo·∫°i model
# ========================================

def create_prompt_template(system_prompt_setup=system_prompt, model_type=None):
    if model_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
    elif model_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        SYSTEM_PROMPT = system_prompt_setup
    elif model_type == "qwen":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
    else:
        SYSTEM_PROMPT = system_prompt_setup
        B_INST, E_INST = "", ""

    instruction = f"""
    Context: {{'history' if history else ''}} \n {{context}}
    User: {{question}}"""

    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
    return PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)


# def model_memory(system_prompt_setup=system_prompt, prompt_template_type=None, history=False):
#     if prompt_template_type == "llama":
#         B_INST, E_INST = "[INST]", "[/INST]"
#         B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
#         SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
#         if history:
#             instruction = """
#             Context: {history} \n {context}
#             User: {question}"""
#
#             prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
#             prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
#         else:
#             instruction = """
#             Context: {context}
#             User: {question}"""
#
#             prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
#             prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
#
#     elif prompt_template_type == "mistral":
#         B_INST, E_INST = "<s>[INST] ", " [/INST]"
#         if history:
#             prompt_template = (
#                 B_INST
#                 + system_prompt_setup
#                 + """
#
#             Context: {history} \n {context}
#             User: {question}"""
#                 + E_INST
#             )
#             prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
#         else:
#             prompt_template = (
#                 B_INST
#                 + system_prompt_setup
#                 + """
#
#             Context: {context}
#             User: {question}"""
#                 + E_INST
#             )
#             prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
#
#     elif prompt_template_type == "qwen":
#         # C·∫•u tr√∫c prompt cho Qwen
#         B_INST, E_INST = "[INST]", "[/INST]"
#         B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
#         SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
#         if history:
#             instruction = """
#             Context: {history} \n {context}
#             User: {question}"""
#
#             prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
#             prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
#         else:
#             instruction = """
#             Context: {context}
#             User: {question}"""
#
#             prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
#             prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
#
#     else:
#         # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
#         if history:
#             prompt_template = (
#                 system_prompt_setup
#                 + """
#
#             Context: {history} \n {context}
#             User: {question}
#             Answer:"""
#             )
#             prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
#         else:
#             prompt_template = (
#                 system_prompt_setup
#                 + """
#
#             Context: {context}
#             User: {question}
#             Answer:"""
#             )
#             prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)
#
#     memory = ConversationBufferMemory(input_key="question", memory_key="history")
#
#     return (
#         prompt,
#         memory,
#     )


# ======================================
# 3. C√°c h√†m ti·ªán √≠ch chung
# ======================================


# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]


# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


def clean_response(response_text):
    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng mong mu·ªën
    unwanted_tags = ["[OUT]", "[INVISIBLE TEXT]", "<<BEGIN>>", "<<END>>"]
    for tag in unwanted_tags:
        response_text = response_text.replace(tag, "")
    # X√≥a c√°c kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    response_text = response_text.strip()
    return response_text


# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"


# ============================================
# 4. Ph·∫ßn ch√≠nh c·ªßa giao di·ªán ·ª©ng d·ª•ng Streamlit
# ============================================


# Sidebar b√™n c·∫°nh tr√°i.
with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n. ")
    st.title("B·∫£o m·∫≠t v√† ri√™ng t∆∞, ho√†n to√†n n·ªôi b·ªô.")

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA kh·∫£ d·ª•ng: {'C√≥' if cuda_available else 'Kh√¥ng'}")
        st.write(f"T·ªïng dung l∆∞·ª£ng VRAM: {total_vram:.2f} GB" if total_vram else "Kh√¥ng th·ªÉ l·∫•y th√¥ng tin VRAM")
        st.write(f"Phi√™n b·∫£n CUDA: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng c·ªßa:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát Ô∏èb·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    st.markdown("- [Streamlit](https://streamlit.io/) - Framework x√¢y d·ª±ng ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng h·ªá th·ªëng LLM hi·ªáu qu·∫£.")
    st.markdown("- [HuggingFace](https://huggingface.co/) - C·ªông ƒë·ªìng ph√°t tri·ªÉn m√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Text input for additional keywords
additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) ƒë·ªÉ h·ªá th·ªëng truy v·∫•n c√≥ th√™m d·ªØ ki·ªán v√† t√¨m ki·∫øm ch√≠nh x√°c h∆°n (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, "
    "tu·ª≥ ch·ªçn th√™m.)",
    key="additional_keywords"
)
# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")


# =======================================
# 5. Ph·∫ßn ch√≠nh: T·∫£i m√¥ h√¨nh v√† x·ª≠ l√Ω c√¢u h·ªèi
# =======================================


# Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
load_model_flag = st.checkbox("N·∫°p m√¥ h√¨nh AI (Vui l√≤ng b·∫•m ch·ªçn ƒë·ªÉ tri·ªÉn khai m√¥ h√¨nh AI.)", value=False)

# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p ch∆∞a
    if "model_loaded" not in st.session_state:
        st.warning(
            "Qu√° tr√¨nh kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ ƒëang ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y ·ª©ng d·ª•ng. Vui "
            "l√≤ng kh·ªüi ƒë·ªông quy tr√¨nh v·ªõi n√∫t *N·∫°p M√¥ H√¨nh AI* ·ªü b·∫£ng tr∆∞·ª£t ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")
        st.session_state["model_loaded"] = False

    if not st.session_state["model_loaded"]:
        # Initialize embeddings
        EMBEDDINGS = initialize_component(
            "EMBEDDINGS",
            lambda: HuggingFaceInstructEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={"device": DEVICE_TYPE},
                embed_instruction="Represent the document content for retrieval in Vietnamese:",
                query_instruction="Represent the query content for retrieval in Vietnamese:"
            )
        )

        # Initialize database
        DB = initialize_component(
            "DB",
            lambda: Chroma(
                persist_directory=PERSIST_DIRECTORY,
                embedding_function=EMBEDDINGS,
                client_settings=CHROMA_SETTINGS,
            )
        )

        retrieval_method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n:",
                                    ["similarity - t√¨m th√¥ng tin t∆∞∆°ng t·ª±.", "mmr - t√¨m th√¥ng tin li√™n quan."])
        # L·∫•y gi√° tr·ªã ch√≠nh t·ª´ chu·ªói v·ª´a ch·ªçn b√™n tr√™n
        method_type = retrieval_method.split(" - ")[0]

        top_k = st.number_input(
            "S·ªë l∆∞·ª£ng t√†i li·ªáu t∆∞∆°ng t·ª± s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ (k):",
            min_value=1,
            max_value=50,
            value=20,
            step=1
        )

        fetch_k = st.number_input(
            "Ph·∫°m vi t√¨m ki·∫øm bao nhi√™u m·∫£nh t√†i li·ªáu cho c√¢u h·ªèi c·ªßa b·∫°n (fetch_k):",
            min_value=10,
            max_value=100,
            value=50,
            step=10
        )

        # X·ª≠ l√Ω v·ªõi vi·ªác ng∆∞·ªùi d√πng ch·ªçn hai tr∆∞·ªùng h·ª£p ƒë·ªÉ h·ªèi.
        if method_type == "similarity":
            # Th√™m slider cho ng∆∞·ª°ng t∆∞∆°ng t·ª±
            score_threshold = st.slider(
                "Ng∆∞·ª°ng ƒëi·ªÉm t∆∞∆°ng t·ª± (score_threshold) quy·∫øt ƒë·ªãnh ƒë·ªô ch√≠nh x√°c c·ªßa k·∫øt qu·∫£ t√¨m ki·∫øm. Gi√° tr·ªã c√†ng "
                "cao th√¨ ch·ªâ c√°c t√†i li·ªáu r·∫•t gi·ªëng v·ªõi c√¢u h·ªèi m·ªõi ƒë∆∞·ª£c ch·ªçn.",
                min_value=0.0,
                max_value=1.0,
                value=0.75,
                step=0.05
            )
            RETRIEVER = initialize_component(
                "RETRIEVER",
                lambda: DB.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": top_k, "fetch_k": fetch_k, "score_threshold": score_threshold}
                )
            )
        else:
            mmr_lambda = st.slider(
                "Tr·ªçng s·ªë lambda ƒëi·ªÅu ch·ªânh s·ª± c√¢n b·∫±ng gi·ªØa t√¨m ki·∫øm th√¥ng tin t∆∞∆°ng t·ª± v√† s·ª± ƒëa d·∫°ng trong c√°c k·∫øt "
                "qu·∫£:",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.1
            )
            RETRIEVER = initialize_component(
                "RETRIEVER",
                lambda: DB.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": top_k, "fetch_k": fetch_k, "lambda": mmr_lambda}
                )
            )

        # Initialize LLM
        LLM = initialize_component("LLM", lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID,
                                                             model_basename=MODEL_BASENAME))

        # S·ª≠ d·ª•ng prompt cho Qwen v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
        prompt, memory = create_prompt_template(system_prompt_setup=system_prompt, model_type="qwen")
        if load_model_flag and "QA" not in st.session_state:
            st.session_state["QA"] = RetrievalQA.from_chain_type(
                llm=LLM,
                chain_type="stuff",
                retriever=RETRIEVER,
                return_source_documents=True,
                chain_type_kwargs={"prompt": prompt, "memory": memory},
            )
        QA = st.session_state["QA"]

        # ƒê√°nh d·∫•u m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p
        st.session_state["model_loaded"] = True

        st.success("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p th√†nh c√¥ng.")
    else:
        st.info("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p tr∆∞·ªõc ƒë√≥. Kh√¥ng c·∫ßn n·∫°p l·∫°i.")

# ==========================================
# 6. X·ª≠ l√Ω ƒë·∫ßu v√†o v√† xu·∫•t k·∫øt qu·∫£
# ==========================================


# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if QA is None:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                # T√°ch c√°c t·ª´ kh√≥a d·ª±a tr√™n d·∫•u ph·∫©y
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                # Th√™m t·ª´ kh√≥a v√†o truy v·∫•n ban ƒë·∫ßu
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë∆∞·ª£c n√¢ng c·∫•p
            with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n..."):
                response = QA(enhanced_query)

            answer, docs = response["result"], response["source_documents"]

            cleaned_answer = clean_response(response["result"])
            st.write("### Answer:")
            st.write(cleaned_answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")
        except Exception as e:
            st.error(f"An error occurred while processing the query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")
