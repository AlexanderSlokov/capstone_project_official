import sys
import os

import torch
import streamlit as st
from localGPT_app.run_localGPT import load_model
from langchain.vectorstores import Chroma
from scripts.env_checking import system_check
from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªõi quy·ªÅn truy c·∫≠p v√†o c√°c t√†i li·ªáu ng·ªØ c·∫£nh. B·∫°n ph·∫£i tr·∫£ l·ªùi
c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng s·ª≠ d·ª•ng th√¥ng tin b√™n ngo√†i.
"""

def model_memory(system_prompt_setup=system_prompt, promptTemplate_type=None, history=False):
    if promptTemplate_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        if history:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "qwen":
        # C·∫•u tr√∫c prompt cho Qwen
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    else:
        # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
        if history:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return (
        prompt,
        memory,
    )

# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]

# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")

def clean_response(response_text):
    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng mong mu·ªën
    unwanted_tags = ["[OUT]", "[INVISIBLE TEXT]", "<<BEGIN>>", "<<END>>"]
    for tag in unwanted_tags:
        response_text = response_text.replace(tag, "")
    # X√≥a c√°c kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    response_text = response_text.strip()
    return response_text

# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"

# Sidebar b√™n c·∫°nh tr√°i.
with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n. ")
    st.title("B·∫£o m·∫≠t v√† ri√™ng t∆∞, ho√†n to√†n n·ªôi b·ªô.")

    # Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
    load_model_flag = st.checkbox("N·∫°p m√¥ h√¨nh AI (Vui l√≤ng b·∫•m ch·ªçn ƒë·ªÉ tri·ªÉn khai m√¥ h√¨nh AI.)", value=False)

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA kh·∫£ d·ª•ng: {'C√≥' if cuda_available else 'Kh√¥ng'}")
        st.write(f"T·ªïng dung l∆∞·ª£ng VRAM: {total_vram:.2f} GB" if total_vram else "Kh√¥ng th·ªÉ l·∫•y th√¥ng tin VRAM")
        st.write(f"Phi√™n b·∫£n CUDA: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng c·ªßa:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát Ô∏èb·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    st.markdown("- [Streamlit](https://streamlit.io/) - Framework x√¢y d·ª±ng ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng h·ªá th·ªëng LLM hi·ªáu qu·∫£.")
    st.markdown("- [HuggingFace](https://huggingface.co/) - C·ªông ƒë·ªìng ph√°t tri·ªÉn m√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")


# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Initialize embeddings
    EMBEDDINGS = initialize_component(
        "EMBEDDINGS",
        lambda: HuggingFaceInstructEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"device": DEVICE_TYPE},
            embed_instruction="Represent the document content for retrieval in Vietnamese:",
            query_instruction="Represent the query content for retrieval in Vietnamese:"
        )
    )

    # Initialize database
    DB = initialize_component(
        "DB",
        lambda: Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=EMBEDDINGS,
            client_settings=CHROMA_SETTINGS,
        )
    )

    retrieval_method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n:",
                                ["similarity - t√¨m th√¥ng tin t∆∞∆°ng t·ª±.", "mmr - t√¨m th√¥ng tin li√™n quan."])
    # L·∫•y gi√° tr·ªã ch√≠nh t·ª´ chu·ªói v·ª´a ch·ªçn b√™n tr√™n
    method_type = retrieval_method.split(" - ")[0]

    top_k = st.number_input(
        "S·ªë l∆∞·ª£ng t√†i li·ªáu t∆∞∆°ng t·ª± s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ (k):",
        min_value=1,
        max_value=50,
        value=20,
        step=1
    )

    fetch_k = st.number_input(
        "Ph·∫°m vi t√¨m ki·∫øm bao nhi√™u m·∫£nh t√†i li·ªáu cho c√¢u h·ªèi c·ªßa b·∫°n (fetch_k):",
        min_value=10,
        max_value=100,
        value=50,
        step=10
    )

    # X·ª≠ l√Ω v·ªõi vi·ªác ng∆∞·ªùi d√πng ch·ªçn hai tr∆∞·ªùng h·ª£p ƒë·ªÉ h·ªèi.
    if method_type == "similarity":
        # Th√™m slider cho ng∆∞·ª°ng t∆∞∆°ng t·ª±
        score_threshold = st.slider(
            "Ng∆∞·ª°ng ƒëi·ªÉm t∆∞∆°ng t·ª± (score_threshold) quy·∫øt ƒë·ªãnh ƒë·ªô ch√≠nh x√°c c·ªßa k·∫øt qu·∫£ t√¨m ki·∫øm. Gi√° tr·ªã c√†ng cao th√¨ ch·ªâ c√°c t√†i li·ªáu r·∫•t gi·ªëng v·ªõi c√¢u h·ªèi m·ªõi ƒë∆∞·ª£c ch·ªçn.",
            min_value=0.0,
            max_value=1.0,
            value=0.75,
            step=0.05
        )
        RETRIEVER = initialize_component(
            "RETRIEVER",
            lambda: DB.as_retriever(
                search_type="similarity",
                search_kwargs={"k": top_k, "fetch_k": fetch_k, "score_threshold": score_threshold }
            )
        )
    else:
        mmr_lambda = st.slider(
            "Tr·ªçng s·ªë lambda ƒëi·ªÅu ch·ªânh s·ª± c√¢n b·∫±ng gi·ªØa t√¨m ki·∫øm th√¥ng tin t∆∞∆°ng t·ª± v√† s·ª± ƒëa d·∫°ng trong c√°c k·∫øt qu·∫£:",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1
        )
        RETRIEVER = initialize_component(
            "RETRIEVER",
            lambda: DB.as_retriever(
                search_type="mmr",
                search_kwargs={"k": top_k, "fetch_k": fetch_k, "lambda": mmr_lambda}
            )
        )

    # Initialize LLM
    LLM = initialize_component("LLM", lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID,
                                                         model_basename=MODEL_BASENAME))

    # S·ª≠ d·ª•ng prompt cho Qwen v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
    prompt, memory = model_memory(promptTemplate_type="qwen", history=False)
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )
else:
    st.warning("Qu√° tr√¨nh kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ ƒëang ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y ·ª©ng d·ª•ng. Vui l√≤ng kh·ªüi ƒë·ªông quy tr√¨nh v·ªõi n√∫t *N·∫°p M√¥ H√¨nh AI* ·ªü b·∫£ng tr∆∞·ª£t ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Text input for additional keywords
additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) ƒë·ªÉ h·ªá th·ªëng truy v·∫•n c√≥ th√™m d·ªØ ki·ªán v√† t√¨m ki·∫øm ch√≠nh x√°c h∆°n (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, tu·ª≥ ch·ªçn th√™m.)", key="additional_keywords"
)
# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")

# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if QA is None:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                # T√°ch c√°c t·ª´ kh√≥a d·ª±a tr√™n d·∫•u ph·∫©y
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                # Th√™m t·ª´ kh√≥a v√†o truy v·∫•n ban ƒë·∫ßu
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë∆∞·ª£c n√¢ng c·∫•p
            response = QA(enhanced_query)
            answer, docs = response["result"], response["source_documents"]

            cleaned_answer = clean_response(response["result"])
            st.write("### Answer:")
            st.write(cleaned_answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")
        except Exception as e:
            st.error(f"An error occurred while processing the query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")
