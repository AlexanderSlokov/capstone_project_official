import datetime
import os
import socket
import sys

import streamlit as st
import torch
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma

from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from localGPT_app.run_localGPT import load_model
from localGPT_app.utils import export_to_pdf
from scripts.env_checking import system_check

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """You are a knowledgeable assistant with access to specific context documents. You must answer the
questions only in Vietnamese language. You must answer questions based on the provided context only.
If you cannot answer based on the context, inform the user politely. Do not use any external information."""

# ===========================================
# 1. Ki·ªÉm tra quy·ªÅn truy c·∫≠p (M·∫≠t kh·∫©u v√† IP)
# ===========================================


# Tr·∫°ng th√°i nh·∫≠p m·∫≠t kh·∫©u ƒë√∫ng ho·∫∑c sai
def check_password():
    """H√†m ki·ªÉm tra m·∫≠t kh·∫©u v·ªõi logic c·∫£i ti·∫øn."""
    if "password_attempted" not in st.session_state:
        st.session_state["password_attempted"] = False  # Tr·∫°ng th√°i l·∫ßn th·ª≠ m·∫≠t kh·∫©u ƒë·∫ßu ti√™n

    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False

    if not st.session_state["password_correct"]:
        # Hi·ªÉn th·ªã √¥ nh·∫≠p m·∫≠t kh·∫©u
        password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u:", type="password")

        # Ch·ªâ hi·ªÉn th·ªã th√¥ng b√°o sai m·∫≠t kh·∫©u n·∫øu ƒë√£ th·ª≠ √≠t nh·∫•t 1 l·∫ßn
        if password:
            st.session_state["password_attempted"] = True
            if password == "secure_password":  # Thay b·∫±ng m·∫≠t kh·∫©u th·ª±c t·∫ø
                st.session_state["password_correct"] = True

        if st.session_state["password_attempted"] and not st.session_state["password_correct"]:
            st.error("M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i.")

        return False  # Ch∆∞a x√°c th·ª±c

    return True  # ƒê√£ x√°c th·ª±c


def get_client_ip():
    """L·∫•y ƒë·ªãa ch·ªâ IP c·ªßa host."""
    try:
        # Tr·∫£ v·ªÅ ƒë·ªãa ch·ªâ IP c·ªßa host trong m·∫°ng LAN
        hostname = socket.gethostname()
        client_ip = socket.gethostbyname(hostname)
        return client_ip
    except Exception as get_client_ip_exeption:
        return f"Unknown IP ({get_client_ip_exeption})"


def check_ip_whitelist():
    """H√†m ki·ªÉm tra IP ng∆∞·ªùi d√πng v·ªõi logic c·∫£i ti·∫øn."""
    allowed_ips = ["172.25.224.1", "127.0.0.1"]  # Th√™m IP cho ph√©p t·∫°i ƒë√¢y
    client_ip = get_client_ip()

    if client_ip not in allowed_ips:
        st.error(f"Truy c·∫≠p b·ªã t·ª´ ch·ªëi: IP c·ªßa b·∫°n kh√¥ng ƒë∆∞·ª£c ph√©p truy c·∫≠p.")
        st.stop()


# X√°c th·ª±c m·∫≠t kh·∫©u tr∆∞·ªõc
if not check_password():
    st.stop()

# Ki·ªÉm tra quy·ªÅn truy c·∫≠p theo IP sau khi nh·∫≠p ƒë√∫ng m·∫≠t kh·∫©u
check_ip_whitelist()


# ========================================
# 2. T·∫°o PromptTemplate cho c√°c lo·∫°i model
# ========================================

def create_prompt_template(system_prompt_setup=system_prompt, prompt_template_type=None, history=False):
    if prompt_template_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif prompt_template_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        if history:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif prompt_template_type == "qwen":
        # C·∫•u tr√∫c prompt cho Qwen
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    else:
        # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
        if history:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return (
        prompt,
        memory,
    )


# ======================================
# 3. C√°c h√†m ti·ªán √≠ch chung
# ======================================


# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]


# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


def clean_response(response_text):
    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng mong mu·ªën
    unwanted_tags = ["[OUT]", "[INVISIBLE TEXT]", "<<BEGIN>>", "<<END>>"]
    for tag in unwanted_tags:
        response_text = response_text.replace(tag, "")
    # X√≥a c√°c kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    response_text = response_text.strip()
    return response_text


# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"

# ============================================
# 4. Ph·∫ßn ch√≠nh c·ªßa giao di·ªán ·ª©ng d·ª•ng Streamlit
# ============================================


# Sidebar b√™n c·∫°nh tr√°i.
with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n. ")
    st.title("B·∫£o m·∫≠t v√† ri√™ng t∆∞, ho√†n to√†n n·ªôi b·ªô.")

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA kh·∫£ d·ª•ng: {'C√≥' if cuda_available else 'Kh√¥ng'}")
        st.write(f"T·ªïng dung l∆∞·ª£ng VRAM: {total_vram:.2f} GB" if total_vram else "Kh√¥ng th·ªÉ l·∫•y th√¥ng tin VRAM")
        st.write(f"Phi√™n b·∫£n CUDA: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng c·ªßa:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát Ô∏èb·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    st.markdown("- [Streamlit](https://streamlit.io/) - Framework x√¢y d·ª±ng ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng h·ªá th·ªëng LLM hi·ªáu qu·∫£.")
    st.markdown("- [HuggingFace](https://huggingface.co/) - C·ªông ƒë·ªìng ph√°t tri·ªÉn m√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Text input for additional keywords
additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) ƒë·ªÉ h·ªá th·ªëng truy v·∫•n c√≥ th√™m d·ªØ ki·ªán v√† t√¨m ki·∫øm ch√≠nh x√°c h∆°n (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, "
    "tu·ª≥ ch·ªçn th√™m.)",
    key="additional_keywords"
)
# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")

# =======================================
# 5. Ph·∫ßn ch√≠nh: T·∫£i m√¥ h√¨nh v√† x·ª≠ l√Ω c√¢u h·ªèi
# =======================================


# Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
load_model_flag = st.checkbox("N·∫°p m√¥ h√¨nh AI (Vui l√≤ng b·∫•m ch·ªçn ƒë·ªÉ tri·ªÉn khai m√¥ h√¨nh AI.)", value=False)

# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Kh·ªüi t·∫°o tr·∫°ng th√°i n·∫øu ch∆∞a t·ªìn t·∫°i
    if "model_loaded" not in st.session_state:
        st.session_state["model_loaded"] = False
        st.session_state["QA"] = None

    if not st.session_state["model_loaded"]:
        try:
            # Initialize embeddings
            EMBEDDINGS = initialize_component(
                "EMBEDDINGS",
                lambda: HuggingFaceInstructEmbeddings(
                    model_name=EMBEDDING_MODEL_NAME,
                    model_kwargs={"device": DEVICE_TYPE},
                    embed_instruction="Represent the document content for retrieval in Vietnamese:",
                    query_instruction="Represent the query content for retrieval in Vietnamese:"
                )
            )

            # Initialize database
            DB = initialize_component(
                "DB",
                lambda: Chroma(
                    persist_directory=PERSIST_DIRECTORY,
                    embedding_function=EMBEDDINGS,
                    client_settings=CHROMA_SETTINGS,
                )
            )

            # Ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n
            retrieval_method = st.radio(
                "Ch·ªçn ph∆∞∆°ng ph√°p truy v·∫•n:",
                ["similarity - t√¨m th√¥ng tin t∆∞∆°ng t·ª±.", "mmr - t√¨m th√¥ng tin li√™n quan."]
            )
            method_type = retrieval_method.split(" - ")[0]

            # C√°c tham s·ªë truy v·∫•n
            top_k = st.number_input(
                "S·ªë l∆∞·ª£ng t√†i li·ªáu t∆∞∆°ng t·ª± s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ (k):",
                min_value=1,
                max_value=50,
                value=20,
                step=1
            )
            fetch_k = st.number_input(
                "Ph·∫°m vi t√¨m ki·∫øm bao nhi√™u m·∫£nh t√†i li·ªáu cho c√¢u h·ªèi c·ªßa b·∫°n (fetch_k):",
                min_value=10,
                max_value=100,
                value=50,
                step=10
            )

            # X·ª≠ l√Ω t√πy ch·ªçn similarity ho·∫∑c mmr
            if method_type == "similarity":
                score_threshold = st.slider(
                    "Ng∆∞·ª°ng ƒëi·ªÉm t∆∞∆°ng t·ª± (score_threshold):",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.75,
                    step=0.05
                )
                RETRIEVER = initialize_component(
                    "RETRIEVER",
                    lambda: DB.as_retriever(
                        search_type="similarity",
                        search_kwargs={"k": top_k, "fetch_k": fetch_k, "score_threshold": score_threshold}
                    )
                )
            else:
                mmr_lambda = st.slider(
                    "Tr·ªçng s·ªë lambda:",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.5,
                    step=0.1
                )
                RETRIEVER = initialize_component(
                    "RETRIEVER",
                    lambda: DB.as_retriever(
                        search_type="mmr",
                        search_kwargs={"k": top_k, "fetch_k": fetch_k, "lambda": mmr_lambda}
                    )
                )

            # Initialize LLM
            LLM = initialize_component(
                "LLM",
                lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME)
            )

            # T·∫°o PromptTemplate v√† Memory
            prompt, memory = create_prompt_template(system_prompt_setup=system_prompt,
                                                    prompt_template_type="qwen", history=False)


            # T·∫°o QA Chain
            st.session_state["QA"] = RetrievalQA.from_chain_type(
                llm=LLM,
                chain_type="stuff",
                retriever=RETRIEVER,
                return_source_documents=True,
                chain_type_kwargs={"prompt": prompt, "memory": memory},
            )

            # ƒê√°nh d·∫•u m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p
            st.session_state["model_loaded"] = True
            st.success("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p th√†nh c√¥ng.")
        except Exception as e:
            st.error(f"C√≥ l·ªói x·∫£y ra khi n·∫°p m√¥ h√¨nh: {str(e)}")
            st.session_state["model_loaded"] = False
    else:
        st.info("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p tr∆∞·ªõc ƒë√≥. Kh√¥ng c·∫ßn n·∫°p l·∫°i.")


# ==========================================
# 6. X·ª≠ l√Ω ƒë·∫ßu v√†o v√† xu·∫•t k·∫øt qu·∫£
# ==========================================


# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if not st.session_state.get("model_loaded", False):
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif st.session_state.get("QA") is None:
        st.error("QA ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra qu√° tr√¨nh n·∫°p m√¥ h√¨nh.")
    elif user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                # T√°ch c√°c t·ª´ kh√≥a d·ª±a tr√™n d·∫•u ph·∫©y
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                # Th√™m t·ª´ kh√≥a v√†o truy v·∫•n ban ƒë·∫ßu
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë∆∞·ª£c n√¢ng c·∫•p
            with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n..."):
                response = st.session_state["QA"](enhanced_query)  # L·∫•y QA t·ª´ session_state

            answer, docs = response["result"], response["source_documents"]

            cleaned_answer = clean_response(response["result"])
            st.write("### Answer:")
            st.write(cleaned_answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")

            # L∆∞u l·ªãch s·ª≠ v√†o session
            if "chat_history" not in st.session_state:
                st.session_state["chat_history"] = []
            st.session_state["chat_history"].append((user_query, answer))

        except Exception as e:
            st.error(f"C√≥ l·ªói x·∫£y ra trong khi x·ª≠ l√Ω Query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")


# =========================================
# 7. Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh PDF
# =========================================

if st.button("Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh PDF"):
    if "chat_history" in st.session_state and st.session_state["chat_history"]:
        session_id = datetime.datetime  # T·∫°o ID phi√™n
        pdf_path = export_to_pdf(session_id, st.session_state["chat_history"])
        st.success(f"L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ ƒë∆∞·ª£c xu·∫•t ra PDF: {pdf_path}")
        with open(pdf_path, "rb") as file:
            st.download_button(
                label="T·∫£i xu·ªëng PDF",
                data=file,
                file_name=f"chat_session_{session_id}.pdf",
                mime="application/pdf"
            )
    else:
        st.error("Kh√¥ng c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ xu·∫•t.")
