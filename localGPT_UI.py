import sys
import os

import torch
import streamlit as st
from localGPT_app.run_localGPT import load_model
from langchain.vectorstores import Chroma
from scripts.env_checking import system_check
from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

system_prompt = """You are a knowledgeable assistant with access to specific context documents. You must answer the
questions only in Vietnamese language. You must answer questions based on the provided context only.
If you cannot answer based on the context, inform the user politely. Do not use any external information."""

def model_memory(system_prompt_setup=system_prompt, promptTemplate_type=None, history=False):
    if promptTemplate_type == "llama":
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "mistral":
        B_INST, E_INST = "<s>[INST] ", " [/INST]"
        if history:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                B_INST
                + system_prompt_setup
                + """

            Context: {context}
            User: {question}"""
                + E_INST
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    elif promptTemplate_type == "qwen":
        # C·∫•u tr√∫c prompt cho Qwen
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        SYSTEM_PROMPT = B_SYS + system_prompt_setup + E_SYS
        if history:
            instruction = """
            Context: {history} \n {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            instruction = """
            Context: {context}
            User: {question}"""

            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    else:
        # Default c·∫•u tr√∫c n·∫øu kh√¥ng ch·ªçn model c·ª• th·ªÉ
        if history:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {history} \n {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["history", "context", "question"], template=prompt_template)
        else:
            prompt_template = (
                system_prompt_setup
                + """

            Context: {context}
            User: {question}
            Answer:"""
            )
            prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

    memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return (
        prompt,
        memory,
    )



# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]


# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


with st.sidebar:
    st.title("ü§óüí¨ Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n c·ªßa b·∫°n. B·∫£o m·∫≠t v√† ri√™ng t∆∞, ho√†n to√†n n·ªôi b·ªô.")
    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng c·ªßa:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    # Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
    load_model_flag = st.checkbox("N·∫°p m√¥ h√¨nh AI (Vui l√≤ng b·∫•m ch·ªçn ƒë·ªÉ tri·ªÉn khai m√¥ h√¨nh AI.)", value=False)

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Ki·ªÉm tra m√¥i tr∆∞·ªùng...")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA kh·∫£ d·ª•ng: {'C√≥' if cuda_available else 'Kh√¥ng'}")
        st.write(f"T·ªïng dung l∆∞·ª£ng VRAM: {total_vram:.2f} GB" if total_vram else "Kh√¥ng th·ªÉ l·∫•y th√¥ng tin VRAM")
        st.write(f"Phi√™n b·∫£n CUDA: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    add_vertical_space(5)
    st.write("·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c t·∫°o ra v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán v√† t·ªëi ∆∞u d√†nh cho ng∆∞·ªùi Vi·ªát v·ªõi ‚ù§Ô∏è b·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")
    st.write("D·ª±a tr√™n c√¥ng ngh·ªá c·ªßa:")
    st.markdown("- [Streamlit](https://streamlit.io/) - Framework cho ·ª©ng d·ª•ng web Python d·ªÖ d√†ng.")
    st.markdown("- [LangChain](https://python.langchain.com/) - C√¥ng c·ª• h·ªó tr·ª£ x√¢y d·ª±ng LLM hi·ªáu qu·∫£.")
    st.markdown("- [HuggingFace](https://huggingface.co/) - M√¥ h√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ ti√™n ti·∫øn.")
    st.markdown("- [ChromaDB](https://www.trychroma.com/) - B·ªô m√°y vector database hi·ªán ƒë·∫°i.")
    st.markdown("- [LocalGPT](https://github.com/PromtEngineer/localGPT) - Kh·ªüi ngu·ªìn c·ªßa ·ª©ng d·ª•ng n√†y.")
    add_vertical_space(2)
    st.write("C·∫£m ∆°n t·∫•t c·∫£ c√°c c√¥ng c·ª• m√£ ngu·ªìn m·ªü v√† c·ªông ƒë·ªìng ph√°t tri·ªÉn ƒë√£ h·ªó tr·ª£ ch√∫ng t√¥i t·∫°o n√™n ·ª©ng d·ª•ng n√†y.")

# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"
# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Initialize embeddings
    EMBEDDINGS = initialize_component(
        "EMBEDDINGS",
        lambda: HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": DEVICE_TYPE})
    )

    # Initialize database
    DB = initialize_component(
        "DB",
        lambda: Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=EMBEDDINGS,
            client_settings=CHROMA_SETTINGS,
        )
    )

    # Initialize retriever
    RETRIEVER = initialize_component("RETRIEVER", lambda: DB.as_retriever())

    # Initialize LLM
    LLM = initialize_component("LLM", lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID,
                                                         model_basename=MODEL_BASENAME))

    # S·ª≠ d·ª•ng prompt cho Qwen v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
    prompt, memory = model_memory(promptTemplate_type="qwen", history=False)
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )

    # Initialize QA pipeline
    prompt, memory = model_memory()
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )
else:
    st.warning("Qu√° tr√¨nh kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ ƒëang ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y ·ª©ng d·ª•ng. Vui l√≤ng kh·ªüi ƒë·ªông quy tr√¨nh v·ªõi n√∫t tr√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω truy v·∫•n vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Text input for additional keywords
additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, tu·ª≥ ch·ªçn th√™m)", key="additional_keywords"
)
# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")

# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if QA is None:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                # T√°ch c√°c t·ª´ kh√≥a d·ª±a tr√™n d·∫•u ph·∫©y
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                # Th√™m t·ª´ kh√≥a v√†o truy v·∫•n ban ƒë·∫ßu
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë∆∞·ª£c n√¢ng c·∫•p
            response = QA(enhanced_query)
            answer, docs = response["result"], response["source_documents"]

            # Display the answer
            st.write("### Answer:")
            st.write(answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")
        except Exception as e:
            st.error(f"An error occurred while processing the query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")
