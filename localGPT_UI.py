import sys
import os


import torch
import streamlit as st
from localGPT_app.run_localGPT import load_model
from langchain.vectorstores import Chroma
from scripts.env_checking import system_check
from config.configurations import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


# Function to create prompt and memory for QA
def model_memory():
    template = """Use the following pieces of context to answer the question at the end. You must response in
    Vietnamese language. If you don't know the answer, just say that you don't know, don't try to make up an answer.'

    {context}

    {history}
    Question: {question}
    Helpful Answer:"""

    init_prompt = PromptTemplate(input_variables=["history", "context", "question"], template=template)
    init_memory = ConversationBufferMemory(input_key="question", memory_key="history")

    return init_prompt, init_memory


# Utility function to initialize Streamlit session components
def initialize_component(key, initializer):
    if key not in st.session_state:
        st.session_state[key] = initializer()
    return st.session_state[key]


# Sidebar contents
def add_vertical_space(amount):
    st.markdown(f"{'' * amount}")


with st.sidebar:
    st.title("ü§óüí¨ Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n c·ªßa b·∫°n ·ªü ƒë√¢y.")
    st.markdown(
        """
        ## About
        ·ª®ng d·ª•ng n√†y l√† m·ªôt LLM-powered chatbot ƒë∆∞·ª£c x√¢y d·ª±ng b·ªüi:
        - [Streamlit](https://streamlit.io/)
        - [LangChain](https://python.langchain.com/)
        - [LocalGPT](https://github.com/PromtEngineer/localGPT)
        """
    )

    # Th√™m checkbox ƒë·ªÉ b·∫≠t/t·∫Øt vi·ªác t·∫£i m√¥ h√¨nh
    load_model_flag = st.checkbox("Load Model (Enable for full functionality)", value=False)

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra m√¥i tr∆∞·ªùng
    st.subheader("üîç Environment Check")

    # Ki·ªÉm tra n·∫øu `env_results` ƒë√£ t·ªìn t·∫°i trong session_state
    if "env_results" not in st.session_state:
        st.session_state["env_results"] = system_check()  # L∆∞u k·∫øt qu·∫£ v√†o session_state

    # L·∫•y k·∫øt qu·∫£ t·ª´ session_state
    env_results = st.session_state["env_results"]

    if env_results:
        cuda_available, total_vram, cuda_version = env_results
        st.write(f"CUDA Available: {'Yes' if cuda_available else 'No'}")
        st.write(f"Total VRAM: {total_vram:.2f} GB" if total_vram else "VRAM Info Unavailable")
        st.write(f"CUDA Version: {cuda_version}")
    else:
        st.error("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm tra h·ªá th·ªëng!")

    add_vertical_space(5)
    st.write("ƒê∆∞·ª£c l√†m v·ªõi ‚ù§Ô∏è b·ªüi [Prompt Engineer](https://youtube.com/@engineerprompt)")
    st.write("Ho√†n thi·ªán cho ng∆∞·ªùi Vi·ªát v·ªõi ‚ù§Ô∏è b·ªüi [ƒêinh T·∫•n D≈©ng - Alexander Slokov]("
             "https://github.com/AlexanderSlokov)")


# Determine the device type
if torch.backends.mps.is_available():
    DEVICE_TYPE = "mps"
elif torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"
# Ki·ªÉm tra tr·∫°ng th√°i checkbox tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh
if load_model_flag:
    # Initialize embeddings
    EMBEDDINGS = initialize_component(
        "EMBEDDINGS",
        lambda: HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": DEVICE_TYPE})
    )

    # Initialize database
    DB = initialize_component(
        "DB",
        lambda: Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=EMBEDDINGS,
            client_settings=CHROMA_SETTINGS,
        )
    )

    # Initialize retriever
    RETRIEVER = initialize_component("RETRIEVER", lambda: DB.as_retriever())

    # Initialize LLM
    LLM = initialize_component("LLM", lambda: load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID,
                                                         model_basename=MODEL_BASENAME))

    # Initialize QA pipeline
    prompt, memory = model_memory()
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )

    # Initialize QA pipeline
    prompt, memory = model_memory()
    QA = initialize_component(
        "QA",
        lambda: RetrievalQA.from_chain_type(
            llm=LLM,
            chain_type="stuff",
            retriever=RETRIEVER,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt, "memory": memory},
        )
    )

else:
    st.warning("Qu√° tr√¨nh kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ ƒëang ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y ·ª©ng d·ª•ng. Vui l√≤ng kh·ªüi ƒë·ªông quy tr√¨nh v·ªõi n√∫t tr√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")

# Main localGPT_app title
st.title("LocalGPT - Tr·ª£ l√Ω ƒë·ªçc vƒÉn b·∫£n AI")

# Text input for user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y", key="user_query")

# Text input for additional keywords
additional_keywords = st.text_input(
    "Th√™m t·ª´ kho√° (keywords) (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y, tu·ª≥ ch·ªçn th√™m)", key="additional_keywords"
)
# Th√™m n√∫t b·∫•m ƒë·ªÉ x√°c nh·∫≠n
submit_button = st.button("G·ª≠i c√¢u h·ªèi")

# Process user input and display response ch·ªâ khi QA ƒë∆∞·ª£c kh·ªüi t·∫°o
if submit_button:
    if QA is None:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng b·∫≠t `Load Model` trong sidebar ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    elif user_query.strip():
        try:
            # X·ª≠ l√Ω t·ª´ kh√≥a b·ªï sung (n·∫øu c√≥)
            if additional_keywords.strip():
                # T√°ch c√°c t·ª´ kh√≥a d·ª±a tr√™n d·∫•u ph·∫©y
                keywords = [kw.strip() for kw in additional_keywords.split(",")]
                # Th√™m t·ª´ kh√≥a v√†o truy v·∫•n ban ƒë·∫ßu
                enhanced_query = user_query + " " + " ".join(keywords)
            else:
                enhanced_query = user_query

            # G·ªçi QA v·ªõi truy v·∫•n ƒë∆∞·ª£c n√¢ng c·∫•p
            response = QA(enhanced_query)
            answer, docs = response["result"], response["source_documents"]

            # Display the answer
            st.write("### Answer:")
            st.write(answer)

            # Expandable section for document similarity search
            with st.expander("Document Similarity Search", expanded=True):
                if docs:
                    for i, doc in enumerate(docs):
                        st.markdown(f"### Source Document #{i + 1}")
                        st.text(doc.metadata.get("source", "Unknown Source"))
                        st.text(doc.page_content)
                else:
                    st.info("No similar documents found.")
        except Exception as e:
            st.error(f"An error occurred while processing the query: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi g·ª≠i.")
